)
}
# Chunk 6
# apply to all links
statuses <- sapply(links, check_link)
# combine into a data frame
results <- data.frame(
link = links,
status = statuses,
alive = statuses == 200
)
DT::datatable(results)
# Chunk 7
# retrieve robots file
rbtxt <- robotstxt::get_robotstxt("https://blogs.lshtm.ac.uk/")
# check events path
pthallow <-
paths_allowed(
path = "/rusers/events/",
domain = "https://blogs.lshtm.ac.uk/"
)
# Chunk 8
head(rbtxt)
pthallow
# Chunk 9
# scrape the page
page <- read_html(url)
# extract full text
page_text <- page %>% html_text2()
# split into lines
lines <- strsplit(page_text, "\n")[[1]]
# put in a dataframe
text_df <- data.frame(line = lines, stringsAsFactors = FALSE) %>%
mutate(line_id = row_number())
# Chunk 10
text_df[10:20,]
# Chunk 11
# REGULAR EXPRESSIONS AND DATES !
regex_pattern <- "(\\d{1,2})(st|nd|rd|th)?\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{4})"
date_lines <- text_df %>%
mutate(date_str = str_extract(line, regex_pattern)) %>%
filter(!is.na(date_str)) %>%
mutate(
date_no_suffix = str_remove_all(date_str, "(st|nd|rd|th)"),
date_parsed = dmy(date_no_suffix)
)
# Chunk 12
# tokenize
words <- text_df[11:43,] %>%
unnest_tokens(word, line)
# remove stopwords
data("stop_words")  # loads default stopwords from tidytext pkg
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
filter(!str_detect(word, "^\\d+$")) # remove pure numbers
# find frequencies
freq <- words_clean %>%
count(word, sort = TRUE)
# print top 30
head(freq, 30)
# Chunk 13
# we load all R packages
av_packs <- utils::available.packages(
contrib.url(repos=c("https://cran.r-project.org"), type="both")
)[,"Package"]
# subset the word list with package names
popular_rpack <- freq %>%
filter(word %in% av_packs)
# Chunk 14
get_cranpkgs_by_org <- function(pkgs_df=tools::CRAN_package_db(), authormatch="lshtm") {
pkgs_authormatch <- pkgs_df[grep(authormatch,pkgs_df$`Authors@R`),"Package"]
dlcounts <- cranlogs::cran_downloads(when = "last-month", packages = pkgs_authormatch) |>
aggregate(x=count~package, FUN=sum)
names(dlcounts) <- c("Package","Downloads (last month)")
pkgs <- pkgs_df |>
subset(x=_, Package%in%pkgs_authormatch,
select=c("Package","Maintainer","Authors@R", "Description")) |>
merge(x=_,y=dlcounts)
rownames(pkgs) <- NULL
pkgs$Description <- stringr::str_remove_all(pkgs$Description, "\n")
return(pkgs)
}
# Chunk 15
# pkgs_lsthm <- get_cranpkgs_by_org(authormatch = "lshtm")
org <- c("lshtm","ucl",".gov.uk")
orgregex <- paste0("(",org,")", collapse = "|")
pkgs_ucl_gov_lshtm <- get_cranpkgs_by_org(authormatch=orgregex)
pkgs_lshtm <- get_cranpkgs_by_org(authormatch = "(lshtm)")
npkgs_byorg <- data.frame(
org,
nCRAN = unlist(lapply(lapply(org, grep, x=pkgs_ucl_gov_lshtm$`Authors@R`),length))
)
npkgs_byorg
View(pkgs_lshtm)
unique(pkgs_lshtm$Maintainer)
writeLines(paste0(pkgs_lshtm$Maintainer, collapse="\n"), "lshtm_package_authors.txt")
write.csv(pkgs_lshtm[,c("Package","Maintainer","Description")], "lshtm_package_authors.txt")
write.csv(pkgs_lshtm[,c("Package","Maintainer","Description")], "lshtm_package_authors.csv")
perf_ter_night_2023 <- readRDS("C:/Users/ArturodelaCruz/OneDrive - London School of Hygiene and Tropical Medicine/Projects/ehm_TempMod/processed/performance/perf_ter_night_2023.RDS")
perf_ter_day_2023 <- readRDS("C:/Users/ArturodelaCruz/OneDrive - London School of Hygiene and Tropical Medicine/Projects/ehm_TempMod/processed/performance/perf_ter_day_2023.RDS")
View(perf_ter_day_2023)
View(perf_ter_night_2023)
perf_aqu_day_2023 <- readRDS("C:/Users/ArturodelaCruz/OneDrive - London School of Hygiene and Tropical Medicine/Projects/ehm_TempMod/processed/performance/perf_aqu_day_2023.RDS")
View(perf_aqu_day_2023)
daynight <- c("day","night")
sats <- c("ter","aqu")
satvars <- setNames(
paste0(rep(sats, each = 2), "_", daynight),
rep(c("max", "min"), 2)); # satvars
modtypes <- c("max","min")
seq_years <- c(2023, NA)
dirstuk <- "D:/ST-UK2022/"
# GENERAL FILE PATH GETTING TO BE CLEANED
# METEO VARIABLES
meteo_var_dirs <- dir(paste0(dirstuk, "DATA/", c("ERA_5_Land", "ERA_5"), "/processed/"),
full.names = T)
removemeteo <- c("u_comp", "v_comp", "d2m", "mslp")
meteofiles <- list.files(meteo_var_dirs, full.names = T) %>%
str_subset(paste0(removemeteo, collapse = "|"), negate = TRUE)
# PACKAGES ----
librarian::shelf(
mlr3, mlr3learners, mlr3extralearners, mlr3tuningspaces, mlr3tuning, mlr3verse,
ranger, lightgbm, xgboost, glmnet, nnls,
future, future.apply, lgr, mgcv,
terra, sf, stars, data.table, dplyr, tidyr, stringr,
ggplot2, scattermore, solrad, tictoc
)
dirstuk <- "D:/ST-UK2022/"
# GENERAL FILE PATH GETTING TO BE CLEANED
# METEO VARIABLES
meteo_var_dirs <- dir(paste0(dirstuk, "DATA/", c("ERA_5_Land", "ERA_5"), "/processed/"),
full.names = T)
removemeteo <- c("u_comp", "v_comp", "d2m", "mslp")
meteofiles <- list.files(meteo_var_dirs, full.names = T) %>%
str_subset(paste0(removemeteo, collapse = "|"), negate = TRUE)
# eobsfiles <- dir("D:/DATA/E-OBS/processed/bng/", full.names=T)
# lstfiles <- dir("D:/DATA/LST-MXD21/processed/bng/", full.names=T)
e5lppfiles <- dir("D:/DATA/ERA5Land-PostProcess/processed/bng/",full.names = T)
# LAND
land_var_files <- dir(
paste0(dirstuk, "DATA/",
c("NDVI",#"LAND_COVER","NIGHT_LIGHT","POPULATION",
"ELEVATION"),#,"ROADS","IMPERVIOUSNESS","SEA_DISTANCE"),
"/processed/"), full.names=T, recursive=T) %>% str_subset(.,"\\.nc")
file_paths <- paste0(c(meteofiles, land_var_files, e5lppfiles))
file_sp <- str_subset(file_paths,paste0("_",c(2000:2024),collapse="|"), negate = TRUE) # %>% str_subset(.,"2024",negate=T)
file_sptemp <- setdiff(file_paths,file_sp) #%>% str_subset(.,"2023")
# SPECFIC LST FILEPATHS
# which file paths to discard
file_sp <- str_subset(file_sp,pattern =
"(NIGHT_LIGHT)|(POPULATION)|(ROADS)|(lc2)|(lc9)|(processed/dist_to_sea.nc)",
negate=TRUE)
# which fiels to keep
file_sptemp <- str_subset(file_sptemp,
"(precipitation)|(imp_uk)|(ndvi)|(tsoilmax)|(tsoilmin)|(tskinmax)|(tskinmin)",
negate=F)
filepaths <- c(file_sp,file_sptemp)
# GENERAL FILE PATH GETTING TO BE CLEANED
# METEO VARIABLES
meteo_var_dirs <- dir(paste0(dirstuk, "DATA/", c("ERA_5_Land", "ERA_5"), "/processed/"),
full.names = T)
meteo_var_dirs
dirstuk <- "D:/ST-UK2022/"
# GENERAL FILE PATH GETTING TO BE CLEANED
# METEO VARIABLES
meteo_var_dirs <- dir(paste0(dirstuk, "DATA/", c("ERA_5_Land", "ERA_5"), "/processed/"),
full.names = T)
R.version
R.version
library(osmdata)
# Define the area of interest (e.g., Banjul, The Gambia)
area <- "Banjul, The Gambia"
# Get road data
roads <- opq(bbox = area) %>%
add_osm_feature(key = "highway") %>%
osmdata_sf()
# View structure of retrieved data
print(roads)
# Plot the roads (optional)
library(ggplot2)
ggplot() +
geom_sf(data = roads$osm_lines, color = "blue") +
ggtitle("Roads in Banjul, The Gambia")
library(osmdata)
library(ggplot2)
library(dplyr)
# Define bounding boxes for entire regions (manually adjusted for accuracy)
# Format: c(min_lon, min_lat, max_lon, max_lat)
bbox_basse <- c(-14.3, 13.2, -13.9, 13.6)   # Covers all of Basse region
bbox_brikama <- c(-16.9, 13.0, -15.6, 13.5) # Covers all of Brikama region
# Function to get OSM data for multiple features using bbox
get_osm_features_bbox <- function(bbox, features) {
data_list <- list()
for (feature in features) {
osm_data <- opq(bbox = bbox) %>%
add_osm_feature(key = feature$key, value = feature$value) %>%
osmdata_sf()
data_list[[paste0(feature$key, ":", feature$value)]] <- osm_data
}
return(data_list)
}
# Define expanded feature list
features <- list(
list(key = "highway", value = "*"),         # Roads
list(key = "industrial", value = "*"),      # Industries
list(key = "power", value = "plant"),       # Power plants
list(key = "aeroway", value = "aerodrome"), # Airports
list(key = "amenity", value = "fuel"),      # Fuel stations
list(key = "amenity", value = "parking"),   # Parking lots
list(key = "building", value = "*"),        # Buildings
list(key = "landuse", value = "residential"),
list(key = "landuse", value = "commercial"),
list(key = "landuse", value = "retail"),
list(key = "landuse", value = "industrial"),
list(key = "landuse", value = "railway"),
list(key = "landuse", value = "construction"),
list(key = "landuse", value = "forest"),
list(key = "landuse", value = "grass"),
list(key = "landuse", value = "meadow"),
list(key = "natural", value = "wood"),
list(key = "natural", value = "water"),
list(key = "landuse", value = "farmland"),
list(key = "landuse", value = "vineyard"),
list(key = "landuse", value = "orchard")
)
# Get data for both regions using bounding boxes
basse_data <- get_osm_features_bbox(bbox_basse, features)
brikama_data <- get_osm_features_bbox(bbox_brikama, features)
# Function to plot a specific feature
plot_osm_feature <- function(osm_data, title, color = "red") {
if (!is.null(osm_data$osm_polygons)) {
ggplot() +
geom_sf(data = osm_data$osm_polygons, fill = color, alpha = 0.5) +
ggtitle(title) +
theme_minimal()
} else {
print(paste("No data for", title))
}
}
# Example: Plot roads in Basse
if (!is.null(basse_data[["highway:*"]]$osm_lines)) {
ggplot() +
geom_sf(data = basse_data[["highway:*"]]$osm_lines, color = "red", size = 0.3) +
ggtitle("Roads in Basse Region") +
theme_minimal() %>%
print()
}
# Example: Plot buildings in Brikama
if (!is.null(brikama_data[["building:*"]]$osm_polygons)) {
ggplot() +
geom_sf(data = brikama_data[["building:*"]]$osm_polygons, fill = "grey", alpha = 0.6) +
ggtitle("Buildings in Brikama Region") +
theme_minimal() %>%
print()
}
ggplot() +
geom_sf(data = basse_data[["highway:*"]]$osm_lines, color = "red", size = 0.3) +
ggtitle("Roads in Basse Region") +
theme_minimal() %>%
print()
View(brikama_data)
View(basse_data)
librarian::shelf(
dplyr, data.table,
mlr3verse, xgboost, ranger, lightgbm, mlr3data
)
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
data$date <- lubridate::yday(data$date)
glimpse(data)
dtrain <- xgb.DMatrix(as.matrix(data), label = unlist(data$count), nthread = 2)
# another
tr_data <- slice_sample(data, prop = .5)
te_data <- anti_join(data, tr_data)
Xtrain <- model.matrix(
object=as.formula(
~windspeed+apparent_temperature+temperature+weather+working_day+humidity+hour+holiday),
data=tr_data) #to get model.matrix;
Xtest <- model.matrix(
object=as.formula(
~windspeed+apparent_temperature+temperature+weather+working_day+humidity+hour+holiday),
data=te_data)
xgb.Xtrain<-xgb.DMatrix(Xtrain, base_margin=log(tr_data$date), label=tr_data$count) #to get model.matrix in xgboost format.
xgb.Xtest <- xgb.DMatrix(Xtest, base_margin=log(te_data$date))
xgb.model.1<-xgboost(data=xgb.Xtrain,
params=list(objective="count:poisson"), nrounds=20)
preds <- predict(xgb.model.1, xgb.Xtest)
cor(te_data$count,preds)
# learner
xgb.mod <- lts(lrn("regr.xgboost"))
# hard set/ restrict some parameter searches
# default is max 5000 rounds
xgb.mod$param_set$values$nrounds = to_tune(50,2000)
# task
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
# learner
xgb.mod <- lts(lrn("regr.xgboost"))
# hard set/ restrict some parameter searches
# default is max 5000 rounds
xgb.mod$param_set$values$nrounds = to_tune(50,2000)
# task
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
data$date <- lubridate::yday(data$date)
task_ukdc <- TaskRegr$new(id="uk_deathcount", backend=data, target="count")
task_ukdc$col_roles$offset = "date"
# tuning / resampling
terminator = trm("evals", n_evals=100)
librarian::shelf(
dplyr, data.table,
xgboost, ranger, lightgbm,
mlr3verse, mlr3data, mlr3tuning, mlr3learners, mlr3
)
# CREATE TASK
data = bike_sharing
# CREATE NAIVE MODEL WITH TUNING SPACE
xgb.mod <- lts(lrn("regr.xgboost", objective="count:poisson"))
# hard set/ restrict some parameter searches
# default is max 5000 rounds
xgb.mod$param_set$values$nrounds = to_tune(50,2000)
# CREATE TASK
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
data$date <- lubridate::yday(data$date)
task_ukdc <- TaskRegr$new(id="uk_deathcount", backend=data, target="count")
task_ukdc$col_roles$offset = "date"
# tuning / resampling
tuner = tnr("random_search")
librarian::shelf(
dplyr, data.table, tictoc,
xgboost, ranger, lightgbm,
mlr3verse, mlr3data, mlr3tuning, mlr3learners, mlr3
)
xgb_ti <- ti(
task=task_ukdc,
learner=xgb.mod,
resampling = rsmp("cv",folds=5),
measures = msr("regr.log_likelihood"),
terminator = trm("n_evals", n_evals=100)
)
xgb_ti <- ti(
task=task_ukdc,
learner=xgb.mod,
resampling = rsmp("cv",folds=5),
measures = msr("msr_log_likelihood"),
terminator = trm("n_evals", n_evals=100)
)
DictionaryMeasure
?DictionaryMeasure
xgb_ti <- ti(
task=task_ukdc,
learner=xgb.mod,
resampling = rsmp("cv",folds=5),
measures = msr("regr.rmsle"),
terminator = trm("n_evals", n_evals=100)
)
xgb_ti <- ti(
task=task_ukdc,
learner=xgb.mod,
resampling = rsmp("cv",folds=5),
measures = msr("regr.rmsle"),
terminator = trm("evals", n_evals=100)
)
tic()
tuner$optimize(xgb_ti)
any(is.na(data))
xgb_ti <- ti(
task=task_ukdc,
learner=xgb.mod,
resampling = rsmp("cv",folds=5),
measures = msr("regr.rsq"),
terminator = trm("evals", n_evals=100)
)
tic()
tuner$optimize(xgb_ti)
xgb_ti <- ti(
task=task_ukdc,
learner=xgb.mod,
resampling = rsmp("cv",folds=5),
terminator = trm("evals", n_evals=100)
)
tic()
tuner$optimize(xgb_ti)
# CREATE NAIVE MODEL WITH TUNING SPACE
xgb.mod <- lrn("regr.xgboost", objective="count:poisson")
# CV
rr_xgb = resample(task_ukdc, xgb.mod,resampling = rsmp("holdout"))
rr_xgb$prediction()
task_ukdc$col_roles$offset = "date"
task_ukdc$col_role
task_ukdc$col_roles
# CREATE NAIVE MODEL WITH TUNING SPACE
xgb.mod <- lrn("regr.xgboost")
# CV
rr_xgb = resample(task_ukdc, xgb.mod,resampling = rsmp("cv",3))
# CV
rr_xgb = resample(task_ukdc, xgb.mod,resampling = rsmp("cv",folds=3))
rr_xgb$prediction()
# CREATE NAIVE MODEL WITH TUNING SPACE
xgb.mod <- lrn("regr.xgboost", objective="count:poisson")
xgb.mod$param_set
xgb.mod$param_set$values$max_delta_step = 0.7
xgb.mod$param_set$values
task_ukdc <- TaskRegr$new(id="uk_deathcount", backend=data, target="count")
task_ukdc$col_roles$offset = "date"
# CV
rr_xgb = resample(task_ukdc, xgb.mod,resampling = rsmp("cv",folds=3))
rr_xgb$prediction()
rr_xgb$predictions()
# CREATE NAIVE MODEL WITH TUNING SPACE
xgb.mod <- lrn("regr.xgboost")#, objective="count:poisson")
# hard set/ restrict some parameter searches
# default is max 5000 rounds
xgb.mod$param_set$values$nrounds = to_tune(50,2000)
xgb.mod$param_set$values$max_delta_step = 0.7
# CREATE TASK
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
data$date <- lubridate::yday(data$date)
task_ukdc <- TaskRegr$new(id="uk_deathcount", backend=data, target="count")
task_ukdc$col_roles$offset = "date"
# CV
rr_xgb = resample(task_ukdc, xgb.mod,resampling = rsmp("cv",folds=3))
# CREATE NAIVE MODEL WITH TUNING SPACE
xgb.mod <- lrn("regr.xgboost")#, objective="count:poisson")
xgb.mod$param_set$values$max_delta_step = 0.7
xgb.mod$param_set$values$nrounds
# CREATE TASK
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
data$date <- lubridate::yday(data$date)
task_ukdc <- TaskRegr$new(id="uk_deathcount", backend=data, target="count")
task_ukdc$col_roles$offset = "date"
# CV
rr_xgb = resample(task_ukdc, xgb.mod,resampling = rsmp("cv",folds=3))
rr_xgb$prediction()
?bike_sharing
librarian::shelf(
data.table, dplyr
mlr3, mlr3data, mlr3learners, mlr3tuning, xgboost
librarian::shelf(
data.table, dplyr,
mlr3, mlr3data, mlr3learners, mlr3tuning, xgboost
)
librarian::shelf(
data.table, dplyr,
mlr3, mlr3data, mlr3learners, mlr3tuning, xgboost
)
# make columns numeric split data
data = bike_sharing
data$holiday <- as.numeric(as.factor(data$holiday))
data$season <- as.numeric(as.factor(data$season))
data$working_day <- as.numeric(as.factor(data$working_day))
data$weather <- as.numeric(data$weather)
data$date <- lubridate::yday(data$date)
tr_data <- slice_sample(data, prop = .5)
te_data <- anti_join(data, tr_data)
# make into train/test xgb compatible objects
form <- "~windspeed+apparent_temperature+temperature+weather+working_day+humidity+hour+holiday"
Xtrain <- model.matrix(
object=as.formula(form),
data=tr_data
)
Xtest <- model.matrix(
object=as.formula(form),
data=te_data)
dim(Xtrain)
names(Xtrain)
View(Xtrain)
colnames(Xtrain)
# make into xgb compatible Dense Matrix and add OFFSET and TARGET columns
xgb.Xtrain<-xgb.DMatrix(Xtrain, base_margin=log(tr_data$date), label=tr_data$count)
xgb.Xtest <- xgb.DMatrix(Xtest, base_margin=log(te_data$date))
xgb.mod<-xgboost(data=xgb.Xtrain,
params=list(objective="count:poisson"),
nrounds=50, max_delta_step=0.7)
preds <- predict(xgb.mod, xgb.Xtest)
summary(te_data$count~preds)
summary(lm(te_data$count~preds))
xgb.mod<-xgboost(data=xgb.Xtrain,
params=list(objective="count:poisson", eval_metric = "logloss"),
max_delta_step=0.7, # this parameter is set to 0.7 when objective = count:poisson
nrounds=50)
xgb.mod<-xgboost(data=xgb.Xtrain,
params=list(objective="count:poisson"),
max_delta_step=0.7, # this parameter is set to 0.7 when objective = count:poisson
nrounds=50)
xgb.mod<-xgboost(data=xgb.Xtrain,
params=list(objective="count:poisson"),
max_delta_step=0.7, # this parameter is set to 0.7 when objective = count:poisson
nrounds=100)
setwd("C:/Users/ArturodelaCruz/OneDrive - London School of Hygiene and Tropical Medicine/Areas/RDrive/pages_and_apps/ehm-lab.github.io")
setwd("C:/Users/ArturodelaCruz/OneDrive - London School of Hygiene and Tropical Medicine/Areas/RDrive/pages_and_apps/ehm-lab.github.io")
